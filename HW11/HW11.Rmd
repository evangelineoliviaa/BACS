---
title: "HW11"
author: '109006206'
output: pdf_document
---

## Set Working Directories & Reading Files
<br>

```{r,message=FALSE,out.width="80%"}
setwd("/Users/olivia/Documents/Documents/Study/Semester 6/BACS/HW11")
library(ggplot2)
library(ggpubr)
library(car)
cars<-read.table("auto-data.txt", header=FALSE, na.strings = "?")
names(cars) <- c("mpg", "cylinders", "displacement", "horsepower", "weight", 
                 "acceleration", "model_year", "origin", "car_name")
```

## QUESTION 1
<br>

```{r,message=FALSE,out.width="80%"}
cars_log <- with(cars, data.frame(log(mpg), log(cylinders), log(displacement), log(horsepower), log(weight), log(acceleration), model_year, origin))
```

#### A) Run a new regression on the cars_log dataset, with mpg.log. dependent on all other variables

#### 1) Which log-transformed factors have a significant effect on log.mpg. at 10% significance?

\
```{r,message=FALSE,out.width="80%"}
cars_regr_log=lm(log.mpg. ~ 
              log.cylinders.+log.displacement.+
              log.horsepower.+log.weight.+log.acceleration.+
              model_year+factor(origin), data = cars_log) 

summary(cars_regr_log)
```
\

**Answer** : The log-transformed factors have a significant effect on log mpg at 10% significance are \color{red}horsepower, weight, acceleration, model_year, origin

\color{black}
#### 2) Do some new factors now have effects on mpg, and why might this be?

\

**Answer** : Yes , horsepower and acceleration now have effects on mpg

#### 3) Which factors still have insignificant or opposite (from correlation) effects on mpg? Why might this be?

\

**Answer** : log.cylinders , log.displacement

#### B) Let’s take a closer look at weight, because it seems to be a major explanation of mpg

#### 1) Create a regression (call it regr_wt) of mpg over weight from the original cars dataset

\
```{r,message=FALSE,out.width="80%"}
regr_wt <- lm(mpg ~ weight, data = cars)
summary(regr_wt)
```

#### 2) Create a regression (call it regr_wt_log) of log.mpg. on log.weight. from cars_log

\
```{r,message=FALSE,out.width="50%"}
regr_wt_log <- lm(log.mpg. ~ log.weight., data = cars_log)
summary(regr_wt_log)
```

\newpage
#### 3) Visualize the residuals of both regression models (raw and log-transformed):

1. Density plots of residuals.
\
```{r,message=FALSE,out.width="80%"}
par(mfrow = c(1, 2))
A<- ggplot(data.frame(residuals = residuals(regr_wt)), aes(x = residuals)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Density Plot of Residuals for Regression of MPG on Weight (Raw)")
B<- ggplot(data.frame(residuals = residuals(regr_wt_log)), aes(x = residuals)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Density Plot of Residuals for Regression of MPG on Weight (Log-Transformed)")

ggarrange(A,B + rremove("x.text"), 
          ncol = 2, nrow = 1)
```

\newpage

2. Scatterplot of log.weight. vs. residuals
\
```{r,message=FALSE,out.width="80%"}
ggplot(cars_log, aes(x = log.weight., y = residuals(regr_wt_log))) +
  geom_point(alpha = 0.5) +
  labs(x = "Log(Weight)", y = "Residuals", title = "Scatterplot of Log(Weight) vs Residuals for Regression of Log(MPG) on Log(Weight)")

```

#### 4) Which regression produces better distributed residuals for the assumptions of regression?

\
**Answer** : I think Log-Transformed regression produces better distributed residuals

#### 5) How would you interpret the slope of log.weight. vs log.mpg. in simple words?

\
**Answer** : 1% increase in weight leads to 1.0583% decrease in mpg

#### 6) From its standard error, what is the 95% confidence interval of the slope of log.weight. vs log.mpg.?

\
```{r,message=FALSE,out.width="80%"}
confint(regr_wt_log, level = 0.95)
```

\newpage 

## QUESTION 2
<br>

\
```{r,message=FALSE,out.width="80%"}
regr_log <- lm(log.mpg. ~ log.cylinders. + log.displacement. + log.horsepower. +
                 log.weight. + log.acceleration. + model_year +
                 factor(origin), data=cars_log)
```

#### A) Using regression and R2, compute the VIF of log.weight. using the approach shown in class

\
```{r,message=FALSE,out.width="80%"}
weight_regr <- lm(log.weight. ~ log.cylinders. + log.displacement. + log.horsepower. +
                    log.acceleration. + model_year +
                    factor(origin), data=cars_log)

r2 <- summary(weight_regr)$r.squared
vif <- 1 / (1 - r2)
vif
```

#### B) Let’s try a procedure called Stepwise VIF Selection to remove highly collinear predictors. Start by Installing the ‘car’ package in RStudio -- it has a function called vif()

#### 1) Use vif(regr_log) to compute VIF of the all the independent variables

\
```{r,message=FALSE,out.width="80%"}
vif(regr_log)
```

#### 2) Eliminate from your model the single independent variable with the largest VIF score that is also greater than 5

\
```{r,message=FALSE,out.width="80%"}
regr_log <- lm(log.mpg. ~ log.cylinders. + log.horsepower. + log.weight. + log.acceleration. + model_year + factor(origin), data = cars_log)
vif(regr_log)
```

\newpage
#### 3) Repeat steps (i) and (ii) until no more independent variables have VIF scores above 5

\
```{r,message=FALSE,out.width="80%"}
regr_log <- lm(log.mpg. ~ log.cylinders. + log.weight. + log.acceleration. + model_year + factor(origin), data = cars_log)
vif(regr_log)

regr_log <- lm(log.mpg. ~ log.horsepower. + log.weight. + log.acceleration. + model_year + factor(origin), data = cars_log)
vif(regr_log)

regr_log <- lm(log.mpg. ~ log.weight. + log.acceleration. + model_year + factor(origin), data = cars_log)
vif(regr_log)
```

#### 4) Report the final regression model and its summary statistic

\
```{r,message=FALSE,out.width="80%"}
final <- lm(log.mpg. ~ log.weight. + log.acceleration. + model_year + factor(origin), data = cars_log)
summary(final)
```

#### C) Using stepwise VIF selection, have we lost any variables that were previously significant? If so, how much did we hurt our explanation by dropping those variables?

\
```{r,message=FALSE,out.width="80%"}
summary(regr_log)$r.squared 
summary(final)$r.squared
```

\
**Answer** : Yes, It is possible that we have lost some variables that were previously significant in the model. Dropping significant variables may hurt the model's explanatory power, as they may have been important predictors of the dependent variable.

Since the new model has a slight lower R-squared, that means we slightly hurt our explanation by dropping those variables.


#### D) From only the formula for VIF, try deducing/deriving the following:

#### 1) If an independent variable has no correlation with other independent variables, what would its VIF score be? 

\
\
**Answer** : If an independent variable has no correlation with other independent variables, it means that its coefficient of determination($R^2$) will be zero
\

Therefore, the denominator in the VIF formula will be equal to 1, and the VIF score will be 1.


#### 2) Given a regression with only two independent variables (X1 and X2), how correlated would X1 and X2 have to be, to get VIF scores of 5 or higher? To get VIF scores of 10 or higher?

\
\
**Answer** : 
\

1) To get VIF scores of 5 or higher : 
\
Correlation between X1 and X2 would need to be at least 0.8944
\
R = sqrt(1 / (1/5 - 1)) = 0.8944
\

2) To get a VIF score of 10 or higher:
\
Correlation between X1 and X2 would need to be at least 0.9487
\
R = sqrt(1 / (1/10 - 1)) = 0.9487

\newpage

## QUESTION 3
<br>

#### A) Let’s add three separate regression lines on the scatterplot, one for each of the origins. Here’s one for the US to get you started:

\
```{r,message=FALSE,out.width="70%"}
origin_colors = c("blue", "darkgreen", "red")
with(cars_log, plot(log.weight., log.mpg., pch=origin, col=origin_colors[origin]))

cars_us <- subset(cars_log, origin==1)
wt_regr_us <- lm(log.mpg. ~ log.weight., data=cars_us)
abline(wt_regr_us, col=origin_colors[1], lwd=2)

cars_eur=subset(cars_log, origin==2) 
wt_regr_eur=lm(log.mpg. ~ log.weight., data=cars_eur) 
abline(wt_regr_eur, col=origin_colors[2], lwd=2)  

cars_jp=subset(cars_log, origin==3) 
wt_regr_jp=lm(log.mpg. ~ log.weight., data=cars_jp) 
abline(wt_regr_jp, col=origin_colors[3], lwd=2)
```

#### B) Do cars from different origins appear to have different weight vs. mpg relationships?

\
**Answer** : Yes, I think different origins have different relationships between weight and mpg.